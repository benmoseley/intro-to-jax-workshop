{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_AICXg0ypBT"
      },
      "source": [
        "# JAX Workshop\n",
        "\n",
        "## The problem\n",
        "\n",
        "Bitney is a new student. He would like to get some help with navigating around the campus. Every day he decides how many hours he is planning to spend around. There are $K=5$ types of facilities he uses:\n",
        "\n",
        "- 0: Library\n",
        "- 1: Lecture theatre\n",
        "- 2: Mensa\n",
        "- 3: Park\n",
        "- 4: Sports centre\n",
        "\n",
        "For example, on Monday he decided to spend five hours on campus, with his trajectory being\n",
        "\n",
        "$\\mathbf{X}_\\text{Monday} = (0, 0, 2, 3, 0)$,\n",
        "\n",
        "meaning that he first spent two hours at the library, then one hour at the mensa, followed by one hour in the nearby park, and then spent another hour at the library.\n",
        "\n",
        "On Tuesday he spent eight hours on campus:\n",
        "\n",
        "$\\mathbf{X}_\\text{Tuesday} = (2, 1, 1, 2, 1, 0, 0, 4)$.\n",
        "\n",
        "Bitney collected the data from several days and now would like to understand better his behaviour if he plans to spend $n$ hours at the campus.\n",
        "For example, he would like to predict:\n",
        "\n",
        "  - How likely is that he'll visit park?\n",
        "  - How many hours will he spend in the library?\n",
        "  - How many different buildings will he visit?\n",
        "\n",
        "We will help him by building a generative model simulating his past (and, hopefully, future) days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HsXvm-fygrC"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "TRAJECTORIES = [[0, 1], [1, 3], [0, 0], [1, 1], [1, 1], [2, 0], [4, 3], [2, 1], [0, 0], [0, 2], [0, 0, 0], [1, 0, 1], [4, 0, 0], [4, 0, 3], [1, 3, 0], [4, 4, 3, 0], [1, 1, 1, 3], [4, 1, 0, 0], [4, 0, 0, 0], [2, 1, 0, 1], [1, 4, 4, 1, 1], [1, 1, 0, 2, 0], [1, 0, 1, 1, 0], [3, 1, 2, 3, 1], [2, 3, 0, 1, 0], [2, 0, 1, 0, 0], [2, 1, 3, 0, 0], [0, 4, 0, 0, 0], [4, 0, 0, 0, 1, 0], [0, 2, 1, 0, 0, 0], [0, 1, 2, 1, 1, 1], [1, 0, 0, 0, 4, 4], [1, 3, 1, 4, 1, 0], [1, 1, 0, 1, 2, 1], [1, 4, 4, 1, 4, 0], [1, 1, 1, 1, 3, 0], [2, 3, 0, 2, 3, 1], [0, 0, 0, 1, 1, 4], [1, 4, 4, 3, 1, 0], [4, 1, 3, 0, 0, 0], [4, 4, 1, 4, 0, 1], [4, 1, 1, 0, 0, 1], [2, 3, 1, 1, 1, 0], [1, 1, 1, 3, 1, 3], [1, 0, 0, 1, 0, 4], [0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 0, 0], [2, 1, 0, 0, 0, 1], [4, 4, 4, 0, 0, 0, 4], [4, 0, 1, 0, 0, 1, 0], [2, 3, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 4, 0], [1, 3, 0, 0, 0, 1, 1], [1, 0, 0, 1, 1, 1, 4], [0, 0, 1, 1, 1, 1, 2], [4, 0, 1, 1, 0, 0, 0], [2, 0, 0, 0, 1, 4, 4], [1, 4, 0, 1, 1, 1, 0], [1, 1, 0, 2, 1, 3, 0], [1, 0, 3, 4, 0, 0, 1], [2, 1, 1, 1, 3, 1, 1], [1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 4, 3], [1, 0, 0, 4, 1, 1, 2], [2, 1, 0, 0, 2, 0, 0], [2, 0, 1, 0, 2, 3, 4], [4, 0, 0, 0, 4, 4, 0], [1, 1, 1, 3, 1, 4, 1], [4, 1, 2, 3, 1, 3, 0], [1, 3, 1, 1, 0, 0, 1], [1, 0, 4, 1, 4, 1, 0], [1, 0, 0, 0, 1, 1, 0], [1, 0, 0, 3, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 4, 1, 0, 0], [0, 3, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 1, 0], [4, 0, 0, 0, 1, 0, 0, 1], [4, 3, 4, 0, 1, 0, 0, 0], [2, 1, 1, 0, 0, 0, 4, 1], [2, 0, 2, 3, 4, 1, 1, 3], [1, 4, 3, 1, 0, 4, 0, 0], [2, 0, 0, 0, 3, 1, 3, 0], [4, 4, 4, 3, 1, 1, 1, 3], [0, 3, 1, 0, 0, 0, 0, 1], [4, 0, 0, 0, 4, 1, 3, 0], [4, 3, 4, 1, 3, 0, 1, 3], [1, 0, 0, 0, 2, 0, 1, 1], [1, 0, 0, 0, 4, 0, 1, 0], [4, 3, 0, 1, 4, 0, 1, 0, 0, 0, 4], [1, 1, 2, 0, 0, 0, 1, 1, 4, 4, 1], [1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0], [0, 0, 2, 3, 1, 1, 3, 1, 3, 1, 1], [1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 2, 0, 0, 0, 4, 1, 0, 0, 0, 1], [1, 3, 1, 1, 4, 4, 4, 0, 0, 0, 1], [4, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1], [4, 4, 0, 0, 0, 0, 0, 1, 1, 4, 4], [1, 1, 3, 0, 4, 0, 0, 2, 1, 2, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0], [2, 1, 0, 0, 0, 1, 3, 1, 4, 0, 2, 3], [1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 1, 0, 4, 4, 4, 1, 1, 1, 1, 0, 4], [1, 0, 0, 0, 2, 3, 1, 3, 0, 0, 1, 1, 1, 1], [4, 0, 0, 0, 4, 4, 0, 1, 0, 0, 2, 3, 0, 0],]\n",
        "TRAJECTORIES = [jnp.asarray(tr, dtype=int) for tr in TRAJECTORIES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6QhLmzvziHB"
      },
      "source": [
        "**Problem:** How many trajectories have been collected in the data set? How long is the longest trajectory? How long is the shortest trajectory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWeivX6HzWrJ"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError(\"This is the place for your code :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_86pDHJzyV6"
      },
      "source": [
        "## Markov chains\n",
        "\n",
        "We will make the following assumptions on the generative process for the sequence $\\mathbf{X} = (X_0, ..., X_{n-1})$:\n",
        "  - Bitney starts at a state $X_0$ chosen at random from the set $\\{0, 1, ..., K-1\\}$. We allow that some places are more likely to be drawn than others.\n",
        "  - Bitney selects a new place basing only on the previous place, i.e., $P(X_{t+1} \\mid X_t) = P(X_{t+1} \\mid X_0, X_1, \\dots, X_t)$.\n",
        "  - The transition matrix $P(X_{t+1} \\mid X_t)$ does not depend on $t$ or the total number of hours spent on campus $n$.\n",
        "\n",
        "In other words, we assume [a time-homogeneous discrete-time Markov chain](https://en.wikipedia.org/wiki/Discrete-time_Markov_chain#Variations).\n",
        "\n",
        "It is parameterized by:\n",
        "\n",
        "  - A vector $(\\pi_k)$ for $k \\in \\{0, 1, \\dots, K - 1\\}$ such that $\\pi_1 + \\cdots + \\pi_{K-1} = 1$. It parameterizes the distribution of the initial states: $P(X_0 = k) = \\pi_k$.\n",
        "  - A matrix $(T_{xy})$ for $x, y \\in \\{0, 1, \\dots, K-1\\}$ parameterising the transition matrix. Namely, $P( X_{t+1} = y \\mid X_{t} = x ) = T_{xy}$. Note that for each $x$ we have a constraint $T_{x,0} + T_{x,1} + \\cdots + T_{x,K-1}=1$.\n",
        "\n",
        "As typical in statistics and machine learning, it is more convenient to work with log-probabilities, i.e., keep $(\\log \\pi_k)$ and $(\\log T_{xy})$, rather than $(\\pi_k)$ and $(T_{xy})$.\n",
        "\n",
        "Let's simulate some example values, which will be useful for testing purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25-pok6IzFN0"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
        "\n",
        "K = 5  # Number of allowed states\n",
        "\n",
        "# Proportions vector, parameterising P(X_0)\n",
        "PI = jax.random.dirichlet(subkey1, alpha=2.0 * jnp.ones(K))\n",
        "# Transition matrix, parameterising P(X_{t+1} | X_t)\n",
        "TM = jax.random.dirichlet(subkey2, alpha= 2.0 * jnp.ones(K), shape=(K,))\n",
        "\n",
        "print(PI.sum())\n",
        "print(TM.sum(axis=1))\n",
        "\n",
        "LOG_PI = jnp.log(PI)\n",
        "LOG_TM = jnp.log(TM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0dyk9M0zQR"
      },
      "source": [
        "## Log-likelihood\n",
        "\n",
        "We need to estimate $\\pi$ and matrix $T$.\n",
        "To do this we introduce log-likelihood, describing the logarithm of the probability that a trajectory $\\mathbf{X} = (X_0, ..., X_{n-1})$ was sampled from the model with the provided parameters:\n",
        "\n",
        "$$\n",
        "    \\ell(\\pi, T; \\mathbf{X}) = \\log P( \\mathbf{X} \\mid \\pi, T) = \\log \\left( P(X_0\\mid \\pi) \\times \\prod_{t=1}^{n-1} P(X_t \\mid X_{t-1}, T) \\right) = \\log \\pi_{X_0} + \\sum_{t=1}^{n-1} \\log T_{X_{t-1}, X_t}.\n",
        "$$\n",
        "\n",
        "Notice that working with log-probabilities allows us to *sum numbers in reasonable ranges* (we don't expect log-probabilities to be outside of the $-1000$ to $0$ interval), rather than *multiplying a lot of small numbers* (which has a risk to result in $0$, when sufficiently many small floats are multiplied).\n",
        "\n",
        "**Problem:** Implement the loglikelihood function using the `for` loop in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apeq8nbC1UFZ"
      },
      "outputs": [],
      "source": [
        "from jaxtyping import Int, Float, Array\n",
        "\n",
        "\n",
        "def loglikelihood_for_loop(\n",
        "    trajectory: Int[Array, \" n\"],\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    \"\"\"Calculates the likelihood for a single trajectory.\"\"\"\n",
        "    raise NotImplementedError(\"Implement the loglikelihood using the `for` loop.\")\n",
        "\n",
        "\n",
        "answer = -4.21\n",
        "\n",
        "print(f\"Your implementation gave: {loglikelihood_for_loop(TRAJECTORIES[0], LOG_TM, LOG_PI):.2f}. Should give {answer:.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a0L5lIm2hUg"
      },
      "source": [
        "Nice! This is Python code which you can trust. We can use it as a baseline for testing future improvements. Speaking of which...\n",
        "\n",
        "### Does JAX like `for` loops?\n",
        "\n",
        "No. It can tolerate them, but the `for` loops are not really that welcome: they get unrolled into multiple operations.\n",
        "\n",
        "**Problem:** Compare Jaxprs corresponding to a short and a long trajectory. What can you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGHaTNCd16e7"
      },
      "outputs": [],
      "source": [
        "short_trajectory = TRAJECTORIES[0]\n",
        "long_trajectory = TRAJECTORIES[-1]\n",
        "\n",
        "raise NotImplementedError(\"This is the place for your code :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-or_JV3Lzu"
      },
      "source": [
        "We are fortunate that Bitney measures time in hours (so that we don't expect sequences longer than 24). But in case he changes the unit to, say, 5 minutes, we will be in a big trouble!\n",
        "We need to replace the `for` loop using a JAX-native concept.\n",
        "\n",
        "### Using the `jax.lax.scan` loops\n",
        "\n",
        "Fortunately, there is a [`jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html) loop which can be used here!\n",
        "\n",
        "Its signature is, roughly, the following:\n",
        "\n",
        "```python\n",
        "def scan(f, init, xs):\n",
        "  carry = init\n",
        "  ys = []\n",
        "  \n",
        "  for x in xs:\n",
        "    carry, y = f(carry, x)\n",
        "    ys.append(y)\n",
        "\n",
        "  return carry, np.stack(ys)\n",
        "```\n",
        "\n",
        "We can use it to replace the `for` loop by carrying over the previous state.\n",
        "\n",
        "#### First example\n",
        "For example, say that we want to calculate $1^1 + 2^2 + \\cdots + 5^5$. We can do it via the `for` loop or an equivalent `jax.lax.scan`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV1I8FeR3Uij"
      },
      "outputs": [],
      "source": [
        "s = 0\n",
        "for i in range(1, 5 + 1):\n",
        "    s += i**i\n",
        "\n",
        "print(f\"Calculated using `for`: {s}\")\n",
        "\n",
        "def scan_fn(carry, current):\n",
        "    return carry + jnp.power(current, current), None\n",
        "\n",
        "s, _ = jax.lax.scan(scan_fn, 0, jnp.arange(1, 5 + 1))\n",
        "\n",
        "print(f\"Calculated using `jax.lax.scan`: {s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlt8LdoI3el3"
      },
      "source": [
        "#### Second example\n",
        "In the example above we were interested in just the last sum. What if we wanted to calculate the sums $1^1 + 2^2 + \\cdots + n^n$ for all $n\\le 5$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dmKyoQO3bZ7"
      },
      "outputs": [],
      "source": [
        "s = 0\n",
        "ys = []\n",
        "for i in range(1, 5 + 1):\n",
        "    s += i**i\n",
        "    ys.append(s)\n",
        "\n",
        "print(f\"Calculated using `for`: {ys}\")\n",
        "\n",
        "def scan_fn(carry, current):\n",
        "    u = carry + jnp.power(current, current)\n",
        "    return u, u\n",
        "\n",
        "_, ys = jax.lax.scan(scan_fn, 0, jnp.arange(1, 5 + 1))\n",
        "\n",
        "print(f\"Calculated using `jax.lax.scan`: {ys}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9D6oNiP3meI"
      },
      "source": [
        "#### Back to our problem\n",
        "\n",
        "Now we are ready to implement the loglikelihood calculation replacing the `for` loop with an appropriate `jax.lax.scan`.\n",
        "\n",
        "**Problem:** Write the body of the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmokiyvG3jF0"
      },
      "outputs": [],
      "source": [
        "def loglikelihood_scan(\n",
        "    trajectory: Int[Array, \" n\"],\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    \"\"\"Calculates the likelihood for a single trajectory using `jax.lax.scan`.\"\"\"\n",
        "\n",
        "    def scan_fn(carry, current):\n",
        "        raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "    # The carry will be the previous state and the sum of the previous log-likelihoods\n",
        "    initial_carry = (trajectory[0], log_pi[trajectory[0]])\n",
        "    final_carry, _ = jax.lax.scan(scan_fn, initial_carry, trajectory[1:])\n",
        "\n",
        "    return final_carry[1]\n",
        "\n",
        "\n",
        "# Test if the implementation agrees with the previous one:\n",
        "for index in [0, 4, 10, -1]:\n",
        "    trajectory = TRAJECTORIES[index]\n",
        "    ll_for = loglikelihood_for_loop(trajectory, LOG_TM, LOG_PI)\n",
        "    ll_scan = loglikelihood_scan(trajectory, LOG_TM, LOG_PI)\n",
        "    print(f\"Loglikelihood comparison: {ll_for:.2f} =?= {ll_scan:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrPCLcAw38A7"
      },
      "source": [
        "Nice, we have a JAX-compatible loglikelihood!\n",
        "\n",
        "**Problem:** How does the Jaxpr of the new implementation look like? Does its length change with the length of the trajectory?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP99tIWo33ID"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError(\"This is the place for your code :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7UQL8fB4J5F"
      },
      "source": [
        "## Total loglikelihood\n",
        "\n",
        "We have defined the loglikelihood function for a single data point. As in our case we have multiple trajectories, we should use all of them. Assuming that trajectories are independent given the model parameters, we have:\n",
        "$$\n",
        "    P\\left( \\mathbf{X}^{(1)}, \\dotsc, \\mathbf{X}^{(M)} \\mid \\pi, T \\right) = \\prod_{m=1}^M P( \\mathbf{X}^{(m)} \\mid \\pi, T),\n",
        "$$\n",
        "\n",
        "so that the total loglikelihood is given by the sum of individual loglikelihoods:\n",
        "\n",
        "\n",
        "$$\n",
        "    \\ell(\\pi, T; \\text{all data}) = \\sum_{m=1}^M  \\ell(\\pi, T; \\mathbf{X}^{(m)}).\n",
        "$$\n",
        "\n",
        "Summation in JAX is easy, isn't it? We can use [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) to calculate loglikelihoods for individual trajectories and then sum them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrRyWFVn4HJe"
      },
      "outputs": [],
      "source": [
        "def calculate_total_loglikelihood_simple_vmap(\n",
        "    trajectories,\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    # Apply loglikelihood for different trajectories, using the same `log_tm` and `log_pi`:\n",
        "    lls = jax.vmap(loglikelihood_scan, in_axes=(0, None, None))(trajectories, log_tm, log_pi)\n",
        "    return jnp.sum(lls)\n",
        "\n",
        "\n",
        "# Check if it works on a few trajectories\n",
        "calculate_total_loglikelihood_simple_vmap(\n",
        "    jnp.asarray(TRAJECTORIES[:5]),\n",
        "    LOG_TM,\n",
        "    LOG_PI,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBqfWNT54bc3"
      },
      "source": [
        "Let's now calculate the loglikelihood for all trajectories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAILdA3A4bL2"
      },
      "outputs": [],
      "source": [
        "calculate_total_loglikelihood_simple_vmap(\n",
        "    jnp.asarray(TRAJECTORIES),\n",
        "    LOG_TM,\n",
        "    LOG_PI,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLydVx5Q4gwB"
      },
      "source": [
        "Oh no, an error! Recall that the `trajectories` is a *list* of arrays which have different lengths. We cannot use `jnp.asarray` to wrap them into a single array...\n",
        "\n",
        "### The `for` loop to the rescue\n",
        "\n",
        "Perhaps we should just use the `for` loop to iterate over the trajectories. Of course, the corresponding Jaxpr is going to be scary, but:\n",
        "  - (a) We'll have a working \"baseline\" solution, which we can use to test any further improvements.\n",
        "  - (b) For just a few trajectories this *may actually be fine*. Later we'll learn the padding technique (which is the preferred solution in this case), but which is not universal – you may work on interesting problems in which padding won't be possible. Then one either has to accept a scary Jaxpr or switch to another fast language, which works better with objects of different lengths (e.g., Rust or Julia).\n",
        "\n",
        "**Problem:** Calculate the total loglikelihood using the `for` loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU9j71vh4usk"
      },
      "outputs": [],
      "source": [
        "def calculate_total_loglikelihood_for_loop(\n",
        "    trajectories: list,\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "print(f\"Total loglikelihood: {float(calculate_total_loglikelihood_for_loop(TRAJECTORIES, LOG_TM, LOG_PI)):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghfLPdl75Fv6"
      },
      "source": [
        "We can check how fast this is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANRLjTsd5CSY"
      },
      "outputs": [],
      "source": [
        "%time calculate_total_loglikelihood_for_loop(TRAJECTORIES, LOG_TM, LOG_PI).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipgLZYS5MI7"
      },
      "source": [
        "Ok, this actually takes quite a long time. However, if we know that the trajectories won't change often (e.g., we are using full-batch gradient descent, rather than stochastic gradient descent for optimisation), we can compile with JIT a [partial application](https://en.wikipedia.org/wiki/Partial_application):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9GPy21s5M3f"
      },
      "outputs": [],
      "source": [
        "# Fix the first argument\n",
        "@jax.jit\n",
        "def f(\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    return calculate_total_loglikelihood_for_loop(TRAJECTORIES, log_tm, log_pi)\n",
        "\n",
        "\n",
        "# Run it once to compile the function\n",
        "print(f\"Total loglikelihood: {float(f(LOG_TM, LOG_PI)):.2f}\")\n",
        "\n",
        "# Run it several times\n",
        "%timeit f(LOG_TM, LOG_PI).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiTVdZI55r6G"
      },
      "source": [
        "This is not bad – doing a few millions of gradient steps is manageable.\n",
        "Before we go any further, let's quickly take a look at another implementation.\n",
        "\n",
        "### Digression: traversing Pytrees\n",
        "\n",
        "The `TRAJECTORIES` object is a `list` of arrays. Nested lists, tuples, and dictionaries, which leaves are arrays, are called [Pytrees](https://jax.readthedocs.io/en/latest/pytrees.html).\n",
        "\n",
        "We can convert a Pytree of trajectories into a Pytree of loglikelihoods using `jax.tree.map` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6V6C0vG5y3c"
      },
      "outputs": [],
      "source": [
        "lls = jax.tree.map(lambda xs: loglikelihood_scan(xs, LOG_TM, LOG_PI), TRAJECTORIES)\n",
        "print(lls[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC7zQfTt51oO"
      },
      "source": [
        "We can sum these numbers by using the `jax.tree.reduce` operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV7Pm0qe56Pk"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "print(jax.tree.reduce(operator.add, lls, initializer=0.0))\n",
        "print(sum(lls, 0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syUaHnCf5-LE"
      },
      "source": [
        "The corresponding implementation of the total loglikelihood looks then as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GddIk44o5-zd"
      },
      "outputs": [],
      "source": [
        "def calculate_total_loglikelihood_tree_map(\n",
        "    trajectories: list,\n",
        "    log_tr: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    lls = jax.tree.map(lambda xs: loglikelihood_scan(xs, log_tr, log_pi), trajectories)\n",
        "    return jax.tree.reduce(operator.add, lls, initializer=0.0)\n",
        "\n",
        "@jax.jit\n",
        "def f(\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    return calculate_total_loglikelihood_tree_map(TRAJECTORIES, log_tm, log_pi)\n",
        "\n",
        "\n",
        "# Run it once to compile the function\n",
        "print(f\"Total loglikelihood: {float(f(LOG_TM, LOG_PI)):.2f}\")\n",
        "\n",
        "# Run it several times\n",
        "%timeit f(LOG_TM, LOG_PI).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxKuSal66CGL"
      },
      "source": [
        "Is it better than using a `for` loop? In this case (a flat list with many items) I would say it's pretty similar: the execution time is similar and the Jaxpr is similarly complicated, so in this case it may not be worth the effort.\n",
        "\n",
        "However, if you have a more complex Pytree (usually with just a few nodes, e.g., nested tuples and dictionaries), you may prefer to use the Pytree-based utilities over traversing the tree manually. You can see how [Equinox](https://docs.kidger.site/equinox/all-of-equinox/) uses Pytrees to create convenient structures for holding e.g., neural network parameters. There is also an [equinox.tree_at](https://docs.kidger.site/equinox/api/manipulation/#equinox.tree_at) utility.\n",
        "\n",
        "### Summary\n",
        "\n",
        "We have seen how to calculate total loglikelihood in two manners: using `for` loop or the Pytree utilities.\n",
        "Both solutions are quite fine when we have small data, so we can keep them in memory and still have a reasonably fast likelihood computation.\n",
        "You may encounter problems which are solvable *only* in this manner.\n",
        "\n",
        "However, in many cases there is a better solution, employing padding.\n",
        "This can be applied in *many* popular settings which have objects of variable size (time series, sequences, text, graphs...).\n",
        "This can give us performance gains (on full batches) and be compatible with data subsampling and SGD, which may be the only strategy viable to use when analysing large data sets.\n",
        "\n",
        "Let's understand the...\n",
        "\n",
        "## Padding trick\n",
        "\n",
        "We couldn't use `jax.vmap` due to the fact that our `TRAJECTORIES` object is a Pytree which can't really be converted to a single array.\n",
        "\n",
        "However, we know that a single trajectory shouldn't be longer 24 hours. Hence, we can pad a trajectory $(X_0, X_1, ..., X_5)$ to, say, size 24 by adding artificial states $-1$: $(X_0, X_1, ..., X_5, -1, -1, -1)$.\n",
        "\n",
        "Thanks to this we can wrap all the trajectories an array and use `jax.vmap`! Because all the trajectories will be then of the same length, we will be also able to create batches with constant shape `(BATCH_SIZE, 24)`, which are compatible with SGD.\n",
        "\n",
        "This is the dream. However, our loglikelihood function doesn't really know that the states $-1$ should not contribute to the loglikelihood. Let's think how we can fix this situation.\n",
        "\n",
        "### Changing the loglikelihood\n",
        "\n",
        "**Problem:** Write a function calculating the loglikelihood ignoring the transitions to the $-1$ state (i.e., increase the loglikelihood by 0, rather than the value from the transition matrix).\n",
        "\n",
        "Hint: You may find [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) or [`jax.lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select) useful:\n",
        "\n",
        "```python\n",
        "jnp.where(True, 3.0, 0.0)        # 3.0\n",
        "jnp.where(False, 3.0, 0.0)       # 0.0\n",
        "\n",
        "jax.lax.select(True, 3.0, 0.0)   # 3.0\n",
        "jax.lax.select(False, 3.0, 0.0)  # 0.0\n",
        "```\n",
        "Alternatively, you can use a batched version of `jnp.where` to create a mask:\n",
        "```python\n",
        "jnp.where(jnp.asarray([0, 1, -1, -1]) > -1, 1.0, 0.0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x96X31A965RH"
      },
      "outputs": [],
      "source": [
        "def loglikelihood_scan_compatible_with_padding(\n",
        "    trajectory: Int[Array, \" n\"],\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    \"\"\"Calculates the likelihood for a single trajectory using `jax.lax.scan`.\n",
        "    Compatible with padding with -1 on the right.\n",
        "    \"\"\"\n",
        "\n",
        "    def scan_fn(carry, current):\n",
        "        raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "    initial_carry = (trajectory[0], log_pi[trajectory[0]])\n",
        "    final_carry, _ = jax.lax.scan(scan_fn, initial_carry, trajectory[1:])\n",
        "\n",
        "    return final_carry[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNc939Jc65oG"
      },
      "outputs": [],
      "source": [
        "def pad_trajectory(trajectory: Int[Array, \" n\"], total_length: int) -> Int[Array, \" total_length\"]:\n",
        "    \"\"\"This function pads a `trajectory` to `total_length`.\n",
        "\n",
        "    Note that it's a preprocessing function, which should be used *outside* of JIT.\n",
        "    \"\"\"\n",
        "    if len(trajectory) > total_length:\n",
        "        raise ValueError(\"This trajectory cannot be padded.\")\n",
        "    elif len(trajectory) == total_length:\n",
        "        return trajectory\n",
        "    else:\n",
        "        return jnp.pad(trajectory, (0, total_length - len(trajectory)), mode=\"constant\", constant_values=-1)\n",
        "\n",
        "\n",
        "# Test whether the padded values indeed don't contribute to the loglikelihood\n",
        "for index in [0, 10, 20, 30, -1]:\n",
        "    trajectory = TRAJECTORIES[index]\n",
        "\n",
        "    ll1 = loglikelihood_scan(trajectory, LOG_TM, LOG_PI)\n",
        "\n",
        "    padded = pad_trajectory(trajectory, 24)\n",
        "    ll2 = loglikelihood_scan_compatible_with_padding(padded, LOG_TM, LOG_PI)\n",
        "\n",
        "    print(f\"{ll1:.2f} =?= {ll2:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKeijbM67Xod"
      },
      "source": [
        "### Calculating the loglikelihood\n",
        "\n",
        "It looks like we can finally obtain the total loglikelihood being as JAX-compatible as possible.\n",
        "\n",
        "**Problem:** Create a `PADDED_TRAJECTORIES` array of shape `(len(TRAJECTORIES), 24)`. You can find `jax.tree.map` helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp2GgLav7XHW"
      },
      "outputs": [],
      "source": [
        "PADDED_TRAJECTORIES = raise NotImplementedError(\"This is the place for your code :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDjGeXZF7d5g"
      },
      "source": [
        "As we have the trajectories padded, we can just apply the `jax.vmap`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUT2-xkl7eqA"
      },
      "outputs": [],
      "source": [
        "# As `trajectories` is now an array of fixed size, we can JIT over everything\n",
        "@jax.jit\n",
        "def calculate_total_loglikelihood_the_jax_way(\n",
        "    trajectories: Int[Array, \"batch_size padded_length\"],\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    # Apply loglikelihood for different trajectories, using the same `log_tm` and `log_pi`:\n",
        "    lls = jax.vmap(loglikelihood_scan_compatible_with_padding, in_axes=(0, None, None))(trajectories, log_tm, log_pi)\n",
        "    return jnp.sum(lls)\n",
        "\n",
        "print(f\"Total loglikelihood: {calculate_total_loglikelihood_the_jax_way(PADDED_TRAJECTORIES, LOG_TM, LOG_PI):.2f}\")\n",
        "\n",
        "# Run it several times\n",
        "%timeit calculate_total_loglikelihood_the_jax_way(PADDED_TRAJECTORIES, LOG_TM, LOG_PI).block_until_ready()\n",
        "\n",
        "# We can also do the JIT on the partial function, in the same way as before.\n",
        "@jax.jit\n",
        "def f(\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    return calculate_total_loglikelihood_the_jax_way(PADDED_TRAJECTORIES, log_tm, log_pi)\n",
        "\n",
        "f(LOG_TM, LOG_PI).block_until_ready()  # Run it for the first time to compile\n",
        "\n",
        "# Run it a few times\n",
        "%timeit f(LOG_TM, LOG_PI).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-G8tFpz7itk"
      },
      "source": [
        "This is some visible improvement! It is more efficient, can be easily used with subsampled values, and the Jaxpr is much shorter.\n",
        "\n",
        "**Problem:** Compare the Jaxpr of `calculate_total_loglikelihood_the_jax_way` with either of the previous solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eMXy1qy70MP"
      },
      "source": [
        "### Digression: Isn't padding wasting too much?\n",
        "\n",
        "Padding is a great technique, but can be quite wasteful: the majority of the entries in the padded trajectories correspond to the artificial state.\n",
        "We can calculate the percentage as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6eW87C474Ny"
      },
      "outputs": [],
      "source": [
        "print(f\"Artificial state prevalence: {100 * float((PADDED_TRAJECTORIES < 0).sum() / PADDED_TRAJECTORIES.size):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuK8cycu740J"
      },
      "source": [
        "This is quite a lot! And if we had just a single very long trajectory (length 100, say), we would need to pad all of our short trajectories.\n",
        "\n",
        "Let's optimise it. We can take a look at the length distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNtpiw8f8cKV"
      },
      "outputs": [],
      "source": [
        "lengths = jnp.asarray(jax.tree.map(len, TRAJECTORIES))\n",
        "\n",
        "for i, v in enumerate(jnp.bincount(lengths)):\n",
        "    print(f\"Length {i} trajectories: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv-BD20V8dxs"
      },
      "source": [
        "Padding all trajectories to length 24 is definitely suboptimal.\n",
        "We could use padding to 14, but at the cost of some additional labour, we can optimise even more.\n",
        "\n",
        "We can group the trajectories into several rough sizes and pad them to smaller values.\n",
        "For example, we could split the data into two data sets: of length at most 7 (with padding to length 7) and these of length at least 8 (with padding to 14).\n",
        "\n",
        "Such custom padding solutions, to fit into several sizes, is quite common in some applications, when the shapes of the objects are very diverse and we do not want to waste resources (both time and memory) by padding everything to the largest object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXv74BL98y-n"
      },
      "outputs": [],
      "source": [
        "short_threshold = 7\n",
        "padding_long = max([len(xs) for xs in TRAJECTORIES])\n",
        "\n",
        "SHORT_TRAJECTORIES = list(filter(lambda xs: len(xs) <= short_threshold, TRAJECTORIES))\n",
        "LONG_TRAJECTORIES = list(filter(lambda xs: len(xs) > short_threshold, TRAJECTORIES))\n",
        "\n",
        "PADDED_SHORT = jnp.asarray(jax.tree.map(lambda x: pad_trajectory(x, short_threshold), SHORT_TRAJECTORIES))\n",
        "PADDED_LONG = jnp.asarray(jax.tree.map(lambda x: pad_trajectory(x, padding_long), LONG_TRAJECTORIES))\n",
        "\n",
        "\n",
        "# We can also do the JIT on the partial function, in the same way as before.\n",
        "@jax.jit\n",
        "def f(\n",
        "    log_tm: Float[Array, \"K K\"],\n",
        "    log_pi: Float[Array, \" K\"],\n",
        ") -> float:\n",
        "    short = calculate_total_loglikelihood_the_jax_way(PADDED_SHORT, log_tm, log_pi)\n",
        "    long = calculate_total_loglikelihood_the_jax_way(PADDED_LONG, log_tm, log_pi)\n",
        "    return short + long\n",
        "\n",
        "\n",
        "f(LOG_TM, LOG_PI).block_until_ready()  # Run it for the first time to compile\n",
        "\n",
        "# Run it a few times\n",
        "%timeit f(LOG_TM, LOG_PI).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9G43z285hS"
      },
      "source": [
        "In Bitney's case (all the trajectories are quite short), the difference is marginal: however, when one considers trajectories of very different lengths (e.g., adding just a few trajectories of length 100) the difference can be much larger.\n",
        "\n",
        "## Optimising the parameters\n",
        "\n",
        "We have define the total loglikelihood. Now we can use it to find the parameters maximising it. Note that we need to optimise over `log_tm` and `log_pi` matrices, which have the following constraints:\n",
        "  - Probabilities sum up to 1.\n",
        "  - Probabilities are between 0 and 1, so that the entries of the log-matrices are negative.\n",
        "\n",
        "\n",
        "One of the simplest solutions is to use [`jax.nn.log_softmax`](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.log_softmax.html) to transform an unconstrained length-$K$ vector to `log_pi` and the rows of an unconstrained $K\\times K$ matrix to `log_tm`. Note that this parameter is not identifiable, but we can still hope that it may perhaps find the loss basing corresponding to a good estimate of interpretable parameters.\n",
        "\n",
        "**Problem:** Estimate `log_tm` and `log_pi` by maximising the likelihood calculated with respect to an unconstrained parameter. Save them to arrays `OPTIMAL_LOG_TM` and `OPTIMAL_LOG_PI`.\n",
        "\n",
        "Hint: You may use [Optax](https://optax.readthedocs.io/en/latest/) or [Optimistix](https://docs.kidger.site/optimistix/) to work with e.g., a dictionary of parameters, storing the parameters of for `log_pi` and `log_tm`, or wrap these parameters into a vector of length $(K+1)K$ compatible with [`jax.scipy.optimize.minimize`](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html).\n",
        "For the latter approach, you may find the following code useful:\n",
        "\n",
        "```python\n",
        "def param_to_log_pi(param: Float[Array, \" (K+1)*K\"]) -> Float[Array, \" K\"]:\n",
        "    param = param.reshape((K+1, K))\n",
        "    return jax.nn.log_softmax(param[0, :])\n",
        "\n",
        "\n",
        "def param_to_log_tm(param: Float[Array, \" (K+1)*K\"]) -> Float[Array, \"K K\"]:\n",
        "    param = param.reshape((K+1, K))\n",
        "    return jax.nn.log_softmax(param[1:, :], axis=1)\n",
        "\n",
        "\n",
        "def loss(param: Float[Array, \" (K+1)*K\"]) -> float:\n",
        "    log_pi = param_to_log_pi(param)\n",
        "    log_tm = param_to_log_tm(param)\n",
        "\n",
        "    short_ll = calculate_total_loglikelihood_the_jax_way(PADDED_SHORT, log_tm, log_pi)\n",
        "    long_ll = calculate_total_loglikelihood_the_jax_way(PADDED_LONG, log_tm, log_pi)\n",
        "    return -(short_ll + long_ll)  # Note the minus sign, to change from likelihood maximisation to loss minimisation.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhSNHXP_8_rf"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError(\"This is the place for your code :)\")\n",
        "OPTIMAL_LOG_TM = None\n",
        "OPTIMAL_LOG_PI = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J54a9edkCG0k"
      },
      "source": [
        "Let's compare if the found values are *somewhat close* to the following values (they won't be exactly the same). If they are *very different*, perhaps running the optimisation for a bit longer may help:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0arGp_BF93mw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "REF_LOG_TM = jnp.asarray([[ -0.51,  -1.61,  -3.  ,  -3.  ,  -2.3 ], [ -1.2 ,  -0.92,  -3.  ,  -1.9 ,  -2.3 ], [ -1.2 ,  -1.2 , -20.72,  -0.92, -20.72], [ -0.8 ,  -0.8 ,  -3.  , -20.72,  -3.  ], [ -1.2 ,  -1.2 , -20.72,  -2.3 ,  -1.2 ]])\n",
        "REF_LOG_PI = jnp.asarray([-1.39, -0.97, -1.9 , -3.91, -1.61])\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(5, 1.5), dpi=200)\n",
        "\n",
        "\n",
        "ax = axs[0]\n",
        "sns.heatmap(REF_LOG_TM, ax=ax, cmap=\"magma\", vmin=-5, vmax=0)\n",
        "ax.set_title(\"Reference $(\\\\log T_{xy})$\")\n",
        "\n",
        "ax = axs[1]\n",
        "sns.heatmap(OPTIMAL_LOG_TM, ax=ax, cmap=\"magma\", vmin=-5, vmax=0)\n",
        "ax.set_title(\"Found $(\\\\log T_{xy})$\")\n",
        "\n",
        "ax = axs[2]\n",
        "x_axis = jnp.arange(K)\n",
        "\n",
        "ax.plot(x_axis, REF_LOG_PI, c=\"goldenrod\")\n",
        "ax.plot(x_axis, OPTIMAL_LOG_PI, c=\"darkblue\")\n",
        "ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
        "ax.set_title(\"$\\\\log \\\\pi_k$\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzQYEFwAC2lZ"
      },
      "source": [
        "As we have found the values of the parameters, we can use them to generate new data and attempt to predict the future.\n",
        "(Well, under a very, very strong assumption: that the future works in the same manner as the past. More accurate description would be [*retrodict the past*](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html#143_Posterior_Retrodiction_Checks).)\n",
        "\n",
        "Bitney is going to spend tomorrow 7 hours at the campus.\n",
        "We would like to know:\n",
        "\n",
        "  - (a) The probability that he will visit the park tomorrow.\n",
        "  - (b) How many number of hours he will spend in the library.\n",
        "\n",
        "To do that, we can sample a large number (e.g., 1000) of trajectories and estimate these quantities basing on simulations. This is called the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method).\n",
        "\n",
        "### Pseudorandom numbers in JAX\n",
        "\n",
        "JAX generates pseudorandom numbers in a *different* way than NumPy or PyTorch.\n",
        "For the overview of differences, see [this link](https://jax.readthedocs.io/en/latest/random-numbers.html).\n",
        "\n",
        "The most important difference is that there is no generator object, but we rather provide a `key` array. The same key passed twice to the same distribution returns exactly the same output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BwTDcwNEtto"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(2024)\n",
        "\n",
        "probs = jnp.asarray([0.25, 0.25, 0.5])\n",
        "\n",
        "output1 = jax.random.categorical(key, logits=jnp.log(probs))\n",
        "output2 = jax.random.categorical(key, logits=jnp.log(probs))\n",
        "\n",
        "print(f\"{output1} is the same as {output2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Mzz0wIF1EI"
      },
      "source": [
        "Hence, we should treat every `key` as an object which can be used only once. To generate distinct numbers we can split the key into subkeys:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4OJXlLnEvXZ"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(2024)\n",
        "\n",
        "key, *subkeys = jax.random.split(key, 3)\n",
        "# `key` is overridden with a new value\n",
        "# `subkeys` is an array of shape 2 = 3-1.\n",
        "\n",
        "probs = jnp.asarray([0.25, 0.25, 0.5])\n",
        "\n",
        "output1 = jax.random.categorical(subkeys[0], logits=jnp.log(probs))\n",
        "output2 = jax.random.categorical(subkeys[1], logits=jnp.log(probs))\n",
        "\n",
        "print(f\"There is a reasonable probability that first number ({output1}) is different from the second ({output2}).\")\n",
        "\n",
        "# If we want to do any further sampling, we should split `key` again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-T4LCeDHdWx"
      },
      "source": [
        "**Problem:** Implement a function using `for` loop that samples a trajectory of a given length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3KtELJqHuuc"
      },
      "outputs": [],
      "source": [
        "def sample_trajectory_for_loop(subkey, n: int, log_tm: Float[Array, \"K K\"], log_pi: Float[Array, \" K\"]) -> Int[Array, \" n\"]:\n",
        "    \"\"\"Samples a trajectory of length `n` using a `for` loop.\"\"\"\n",
        "    raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "\n",
        "key, subkey = jax.random.split(key)  # By default splits into two\n",
        "\n",
        "trajectory_length = 7\n",
        "print(\"Sampled trajectory:\")\n",
        "print(sample_trajectory_for_loop(subkey, trajectory_length, OPTIMAL_LOG_TM, OPTIMAL_LOG_PI))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCoemmxEuN8"
      },
      "source": [
        "Great! Now we can answer the problem:\n",
        "\n",
        "**Problem:** Bitney is going to spend tomorrow 7 hours at the campus. Generate 1000 trajectories. of length 7 and estimate:\n",
        "\n",
        "  - (a) The probability that he will visit the park (location 3) tomorrow.\n",
        "  - (b) How much time he is expected to spend in the library (location 0) tomorrow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efz1NqYZIutH"
      },
      "outputs": [],
      "source": [
        "def calculate_probability_visit_park(trs: Int[Array, \"batch n\"]) -> float:\n",
        "  \"\"\"Returns the probability that there is at least one \"3\" in the trajectory.\"\"\"\n",
        "  raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "\n",
        "def calculate_mean_number_of_library_hours(trs: Int[Array, \"batch n\"]) -> float:\n",
        "  \"\"\"Returns the expected number of \"0\" entries in a trajectory.\"\"\"\n",
        "  raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "\n",
        "num_trajectories = 1_000\n",
        "\n",
        "key, *subkeys = jax.random.split(key, num_trajectories + 1)\n",
        "trajectories = jnp.asarray([\n",
        "    sample_trajectory_for_loop(subkey, trajectory_length, OPTIMAL_LOG_TM, OPTIMAL_LOG_PI)\n",
        "    for subkey in subkeys\n",
        "])\n",
        "\n",
        "print(f\"Probability of visiting the park: {calculate_probability_visit_park(trajectories):.2f}\")\n",
        "print(f\"Expected library time:            {calculate_mean_number_of_library_hours(trajectories):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l-gmScuGvxo"
      },
      "source": [
        "Great! This works. However, we can speed this up. Currently we are using:\n",
        "\n",
        "  - A `for` loop to sample the trajectories. For long trajectories this is slow.\n",
        "  - A Python list comprehension to sample multiple trajectories of the same length. For a large number of trajectories this is slow.\n",
        "\n",
        "The JAX-native solutions would be:\n",
        "\n",
        "  - A `jax.lax.scan` to replace the `for` loop.\n",
        "  - A `jax.vmap` to replace the list comprehension.\n",
        "\n",
        "**Problem:** Implement a function sampling the trajectories in JAX using `jax.lax.scan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfmciPIzGuvi"
      },
      "outputs": [],
      "source": [
        "def sample_trajectory_the_jax_way(subkey, n: int, log_tm: Float[Array, \"K K\"], log_pi: Float[Array, \" K\"]) -> Int[Array, \" n\"]:\n",
        "    \"\"\"Samples a trajectory of length `n`.\"\"\"\n",
        "    raise NotImplementedError(\"This is the place for your code :)\")\n",
        "\n",
        "\n",
        "key, key2 = jax.random.split(key)\n",
        "subkeys = jax.random.split(key2, num_trajectories)\n",
        "\n",
        "trajectories = jax.vmap(sample_trajectory_the_jax_way, in_axes=(0, None, None, None))(subkeys, 7, OPTIMAL_LOG_TM, OPTIMAL_LOG_PI)\n",
        "\n",
        "\n",
        "print(f\"Probability of visiting the park: {calculate_probability_visit_park(trajectories):.2f}\")\n",
        "print(f\"Expected library time:            {calculate_mean_number_of_library_hours(trajectories):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSIVdh9GLRDy"
      },
      "source": [
        "Hopefully, we see that the estimates of the quantities of interest are similar. As Monte Carlo standard error is scaling as $O(1/\\sqrt{N})$, increasing the number of sampled trajectories can yield closer estimates.\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "Congratulations – you have finished the workshop! In case there are any problems left, you can take a look at the [**solutions**](https://colab.research.google.com/drive/1zL0dLPVFt5pmsqtBQJqNlQxHjsGgcjKm?usp=sharing).\n",
        "\n",
        "To reiterate the most important points from this notebook:\n",
        "\n",
        "  - Do use the `jax.lax` alternatives for the `for` loop and `if/else` whenever possible.\n",
        "  - Start with a Python function you trust, and then use it to test a JAX alternative. I usually put the Python \"baseline\" functions in [the unit tests](https://docs.pytest.org/).\n",
        "  - As JAX works with arrays of fixed shapes, use *padding* to resize the objects into just a few fixed shapes. You may need to write a different function (which ignores the padded values), e.g., by using masking.\n",
        "  - JAX supports operations on Pytrees. For complex data structures, such as these supported by [Equinox](https://docs.kidger.site/equinox/all-of-equinox/) this is great. But remember that a single array is much better than a Pytree consisting of list of arrays.\n",
        "\n",
        "\n",
        "### Where to go from here?\n",
        "\n",
        "Probably the best way to continue the JAX adventure is to start using it in your own projects. However, if you want to start a new side project right now, the following ideas may be interesting:\n",
        "\n",
        "  - Once you have a neural network represented as a Pytree ([Equinox](https://docs.kidger.site/equinox/all-of-equinox/)), you have all the building blocks for building diffusion models. See the [blackout diffusion](https://arxiv.org/abs/2305.11089) paper and implement sampling in JAX (or the training). Other diffusion-based models are also easy to implement in JAX.\n",
        "  - In case you want to drop the time-homogeneous Markov assumption, you may consider an autoregressive model: $P(X_0, \\dotsc, X_{n-1}) = P(X_0)P(X_1\\mid X_0) P(X_2\\mid X_1, X_0) \\cdots P(X_{n-1}\\mid X_{n-2}, \\dots, X_{1}, X_0)$. In this case padding is a typical technique, together with masking. You can take a look at the [JAX-based PaLM implementation](https://github.com/lucidrains/PaLM-jax) or [Levanter](https://github.com/stanford-crfm/levanter), a JAX-based framework for building large language models.\n",
        "  - If you are interested in doing Bayesian inference over the parameters $T$ and $\\pi$, rather than optimisation, see the [NumPyro](https://num.pyro.ai/) and [BlackJAX](https://blackjax-devs.github.io/blackjax/) packages. Below there is a prototype showing how easy Bayesian inference is in JAX (once you can build likelihood function).\n",
        "\n",
        "\n",
        "## Bonus: Bayesian inference in JAX\n",
        "\n",
        "\n",
        "As we have the likelihood, i.e., $P(\\text{data}\\mid \\pi, T)$, if we put a prior $P(\\pi, T)$, we can use the [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_theorem) and obtain the posterior, $P(\\pi, T \\mid \\text{data})$:\n",
        "$$P(\\pi, T \\mid \\text{data}) = \\frac{ P(\\text{data}\\mid \\pi, T)\\, P(\\pi, T) }{P(\\text{data})},$$\n",
        "\n",
        "which quantifies how probable different solutions are. This approach to estimation is called *Bayesian inference* and allows us to express any uncertainty in parameter estimates using the posterior probability distribution.\n",
        "\n",
        "We will put the uniform probability distribution over each simplex, i.e., $\\text{Dirichlet}(1, 1, \\dotsc, 1)$ distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IColTmfIRKZu"
      },
      "outputs": [],
      "source": [
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "\n",
        "def model(K):\n",
        "    # Sample pi from a uniform distribution over the probability simplex\n",
        "    pi = numpyro.sample(\"pi\", dist.Dirichlet(jnp.ones(K)))\n",
        "\n",
        "    # Sample T from K Dirichlet distributions\n",
        "    T = numpyro.sample(\"T\", dist.Dirichlet(jnp.ones(K)).expand([K]))\n",
        "\n",
        "    numpyro.factor(\"likelihood\",\n",
        "        calculate_total_loglikelihood_the_jax_way(PADDED_TRAJECTORIES, log_tm=jnp.log(T), log_pi=jnp.log(pi))\n",
        "    )\n",
        "\n",
        "    return pi, T\n",
        "\n",
        "\n",
        "nuts_kernel = NUTS(model)\n",
        "mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000, num_chains=4)\n",
        "mcmc.run(jax.random.PRNGKey(0), K)\n",
        "mcmc.print_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yLfFW5oCetJ"
      },
      "source": [
        "With just a few lines of code more, we could do Bayesian inference, which provides estimates together with uncertainty!\n",
        "We can use these samples to generate the predictive samples (i.e., sample the trajectories). In many cases, they will show higher variability than these corresponding to a single point estimate.\n",
        "\n",
        "Bayesian data analysis doesn't end up with inference. For discussion of model checking and other topics, we refer to [BDA3](https://stat.columbia.edu/~gelman/book/) and [Statistical rethinking](https://youtu.be/FdnMWdICdRs?feature=shared)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
