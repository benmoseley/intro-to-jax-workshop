{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d15ed39",
   "metadata": {},
   "source": [
    "# Introduction to JAX\n",
    "\n",
    "Written by Ben Moseley, June 2024\n",
    "\n",
    "\n",
    "## What is JAX?\n",
    "\n",
    "<img src=\"what-is-jax.png\" width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=2'\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8302346",
   "metadata": {},
   "source": [
    "# Arrays with JAX NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.array([[0., 2., 4.]])\n",
    "print(x, x.shape)\n",
    "print(x @ x.T)\n",
    "print(x * x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a148018",
   "metadata": {},
   "source": [
    "# Autodifferentiation with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "x = jnp.linspace(-5,5,500)\n",
    "\n",
    "plt.plot(x, fn(x), label=\"f(x)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "# TODO: define dfdx and d2dfdx2 functions\n",
    "\n",
    "plt.plot(x, fn(x), label=\"f(x)\")\n",
    "plt.plot(x, [dfdx_fn(x_) for x_ in x], label=\"df/dx\")\n",
    "plt.plot(x, [d2fdx2_fn(x_) for x_ in x], label=\"d$^2$f/dx$^2$\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d43fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.make_jaxpr(fn)(x))# JAX transforms programs using a simple intermediate language call jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian\n",
    "jacobian_fn = jax.jacfwd(fn)\n",
    "j = jacobian_fn(x)\n",
    "print(j)\n",
    "print(j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector-Jacobian product\n",
    "f, vjp_fn = jax.vjp(fn, x)\n",
    "dfdx, = vjp_fn(jnp.ones_like(x))\n",
    "\n",
    "plt.plot(x, f, label=\"f(x)\")\n",
    "plt.plot(x, dfdx, label=\"df/dx\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "\n",
    "# Jacobian-vector product\n",
    "f, dfdx = jax.jvp(fn, (x,), (jnp.ones_like(x),))\n",
    "\n",
    "plt.plot(x, f, label=\"f(x)\")\n",
    "plt.plot(x, dfdx, label=\"df/dx\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db3256",
   "metadata": {},
   "source": [
    "# Vectorisation with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(w, b, x):\n",
    "    x = w @ x + b\n",
    "    x = jnp.tanh(x)\n",
    "    return x\n",
    "\n",
    "key = jax.random.key(seed=0)\n",
    "key1, key2, key3 = jax.random.split(key, 3)\n",
    "x = jax.random.normal(key1, (3,))\n",
    "w = jax.random.normal(key2, (10,3))\n",
    "b = jax.random.normal(key3, (10,))\n",
    "y = forward_fn(w, b, x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: vectorise forward_fn\n",
    "\n",
    "x_batch = jax.random.normal(key, (1000,3))\n",
    "y_batch = forward_batch_fn(w, b, x_batch)\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05563f",
   "metadata": {},
   "source": [
    "# Just-in-time compilation with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba98473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    return x + x*x + x*x*x\n",
    "\n",
    "jit_fn = jax.jit(fn)\n",
    "\n",
    "x = jax.random.normal(key, (1000,1000))\n",
    "%timeit fn(x).block_until_ready()\n",
    "%timeit jit_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3e59e",
   "metadata": {},
   "source": [
    "# Putting it all together: polynominal regression with JAX\n",
    "\n",
    "The goal of this task is to gain a basic familiarity with JAX's function transforms (`vmap`, `jit`, `grad`).\n",
    "\n",
    "We will carry out polynominal regression, using the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "x_batch = jnp.linspace(-1, 1, 100).reshape((100,1))\n",
    "y_label_batch = 3*x_batch**3 - x_batch**2 - 3*x_batch + 2 + 0.2*jax.random.normal(key, (100,1))\n",
    "\n",
    "plt.scatter(x_batch, y_label_batch, label=\"training data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd01d4",
   "metadata": {},
   "source": [
    "> Task: define a `forward(theta, x)` function, which predicts `y` given a single `x` value and some learnable parameters, `theta`.\n",
    "\n",
    "> Task: use `jax.vmap` to transform `forward` to a function which predicts a batch of `y` values given a batch of `x` values. Test that it works using `x_batch` above.\n",
    "\n",
    "Hint: you can assume that the function is a third order polynominal, and learn its coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(theta, x):\n",
    "    \"Returns model prediction, for a single example input\"\n",
    "    # TODO: write some code\n",
    "    \n",
    "    return y\n",
    "\n",
    "# TODO: write some code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be3d34",
   "metadata": {},
   "source": [
    "> Task: next, define a `loss(theta, x_batch, y_label_batch)` function which computes the mean-squared error of the model.\n",
    "\n",
    "> Task: use `jax.grad` to transform `loss` into a function which computes the gradient of `loss` with respect to the model parameters, `theta`. Test that it outputs an array the same shape as `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(theta, x_batch, y_label_batch):\n",
    "    \"Computes mean squared error between model prediction and training data\"\n",
    "    # TODO: write some code\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# TODO: write some code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0f14c",
   "metadata": {},
   "source": [
    "> Task: define an `step(lrate, theta, x_batch, y_label_batch)` function which carries out a single gradient descent step on `theta`, using the `grad` function you created above. It should return the updated `theta`.\n",
    "\n",
    "> Task: use `jax.jit` to compile `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e66b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(lrate, theta, x_batch, y_label_batch):\n",
    "    \"Performs one gradient descent step on model parameters, given training data\"\n",
    "    # TODO: write some code\n",
    "    \n",
    "    return theta, lossval\n",
    "\n",
    "# TODO: write some code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfef057",
   "metadata": {},
   "source": [
    "> Task: finally, write a python `for` loop to train the model over 1000 gradient descent steps, with a learning rate of 0.1. You are free to choose how to initialise `theta`. Plot the results of the model.\n",
    "\n",
    "> Task: compare the speed of training with/without `jit` compilation. How much faster is the compiled code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa411a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write some code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e53e15",
   "metadata": {},
   "source": [
    "# ðŸ”ª JAX: the sharp bits\n",
    "\n",
    "JAX introduces a number of **restrictions** which are necessary for JAX transformations to work (`vmap`, `grad`, `jit`, etc).\n",
    "\n",
    "Below are most of the major ones. See [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for a full list.\n",
    "\n",
    "## ðŸ”ª Pure functions\n",
    "\n",
    "JAX transforms are designed to work on pure functions.\n",
    "\n",
    "That is, all the input data is passed through the function parameters, and all the results are output through the function results. \n",
    "\n",
    "Otherwise unexpect results might occur! Note: this makes transforming Python classes tricky (but not [impossible](https://jax.readthedocs.io/en/latest/stateful-computations.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "def f(a):\n",
    "    return x+a\n",
    "\n",
    "print(jax.make_jaxpr(f)(1))# when we jit this function, the current value of x is hard-coded into the compiled code!\n",
    "jit_f = jax.jit(f)\n",
    "print(jit_f(1))\n",
    "x = 20\n",
    "print(jit_f(1))# (!) not what you expect (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2fa79",
   "metadata": {},
   "source": [
    "## ðŸ”ªðŸ”ª Static shapes\n",
    "JAX transforms require all output shapes to be known in **advance**, given the input shapes.\n",
    "\n",
    "This is probably the biggest restriction. We can't transform functions which output dynamic shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i):\n",
    "    return jnp.ones(shape=(i,))\n",
    "\n",
    "print(f(jnp.array(10)))\n",
    "#print(jax.jit(f)(jnp.array(10)))# raises a TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f318311",
   "metadata": {},
   "source": [
    "ðŸ”ª This means **dynamic programming** and **sparse (matrix) computation** can often be challenging in JAX (although tricks like masking and compressed representations can be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8cc10",
   "metadata": {},
   "source": [
    "## ðŸ”ª Out-of-place array updates\n",
    "\n",
    "JAX only allows **out-of-place** array updates. \n",
    "\n",
    "This is because allowing mutation of variables in-place makes program analysis and transformation difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "a[0] = -1# you are used to doing this\n",
    "\n",
    "a = jnp.array([1,2,3])\n",
    "#a[0] = 1# will raise a TypeError\n",
    "b = a.at[0].set(-1)# this is the JAX equivalent\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107352f3",
   "metadata": {},
   "source": [
    "## ðŸ”ª Random numbers\n",
    "\n",
    "JAX handles random number generation explicitly ([for good reason](https://jax.readthedocs.io/en/latest/random-numbers.html)). \n",
    "\n",
    "This means the user must explicitly carry around the state of the random number generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efab6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "print(jax.random.normal(key, (1,)))\n",
    "print(jax.random.normal(key, (1,)))# generates same number\n",
    "key, subkey = jax.random.split(key, num=2)# we need to explicitly split the RNG to generate new numbers\n",
    "print(jax.random.normal(subkey, (1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33877a70",
   "metadata": {},
   "source": [
    "# Extra: multi-device parallelisation with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa846bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import PositionalSharding\n",
    "\n",
    "print(jax.devices())\n",
    "\n",
    "x = jax.random.normal(key, (8192, 8192))\n",
    "\n",
    "sharding = PositionalSharding(jax.devices()).reshape(2,1)\n",
    "x = jax.device_put(x, sharding)\n",
    "jax.debug.visualize_array_sharding(x)# shards array across first dimension\n",
    "\n",
    "y = x**2\n",
    "jax.debug.visualize_array_sharding(y)# \"computation follows sharding\" paradigm\n",
    "\n",
    "y = jnp.mean(x**2, axis=0, keepdims=True)# compiler also inserts communication as necessary!\n",
    "jax.debug.visualize_array_sharding(y)# result is replicated across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a182d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
